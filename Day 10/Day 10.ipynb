{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileText = open('day10Input.txt', 'r').read()\n",
    "lines = fileText.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One\n",
    "\n",
    "You ask the submarine to determine the best route out of the deep-sea cave, but it only replies:\n",
    "\n",
    "Syntax error in navigation subsystem on line: all of them\n",
    "All of them?! The damage is worse than you thought. You bring up a copy of the navigation subsystem (your puzzle input).\n",
    "\n",
    "The navigation subsystem syntax is made of several lines containing chunks. There are one or more chunks on each line, and chunks contain zero or more other chunks. Adjacent chunks are not separated by any delimiter; if one chunk stops, the next chunk (if any) can immediately start. Every chunk must open and close with one of four legal pairs of matching characters:\n",
    "\n",
    "- If a chunk opens with (, it must close with ).\n",
    "- If a chunk opens with [, it must close with ].\n",
    "- If a chunk opens with {, it must close with }.\n",
    "- If a chunk opens with <, it must close with >.\n",
    "\n",
    "So, () is a legal chunk that contains no other chunks, as is []. More complex but valid chunks include ([]), {()()()}, <([{}])>, [<>({}){}[([])<>]], and even (((((((((()))))))))).\n",
    "\n",
    "Some lines are incomplete, but others are corrupted. Find and discard the corrupted lines first.\n",
    "\n",
    "A corrupted line is one where a chunk closes with the wrong character - that is, where the characters it opens and closes with do not form one of the four legal pairs listed above.\n",
    "\n",
    "Examples of corrupted chunks include (], {()()()>, (((()))}, and <([]){()}[{}]). Such a chunk can appear anywhere within a line, and its presence causes the whole line to be considered corrupted.\n",
    "\n",
    "For example, consider the following navigation subsystem:\n",
    "```\n",
    "[({(<(())[]>[[{[]{<()<>>\n",
    "[(()[<>])]({[<{<<[]>>(\n",
    "{([(<{}[<>[]}>{[]{[(<()>\n",
    "(((({<>}<{<{<>}{[]{[]{}\n",
    "[[<[([]))<([[{}[[()]]]\n",
    "[{[{({}]{}}([{[{{{}}([]\n",
    "{<[[]]>}<{[{[{[]{()[[[]\n",
    "[<(<(<(<{}))><([]([]()\n",
    "<{([([[(<>()){}]>(<<{{\n",
    "<{([{{}}[<[[[<>{}]]]>[]]\n",
    "```\n",
    "Some of the lines aren't corrupted, just incomplete; you can ignore these lines for now. The remaining five lines are corrupted:\n",
    "```\n",
    "{([(<{}[<>[]}>{[]{[(<()> - Expected ], but found } instead.\n",
    "[[<[([]))<([[{}[[()]]] - Expected ], but found ) instead.\n",
    "[{[{({}]{}}([{[{{{}}([] - Expected ), but found ] instead.\n",
    "[<(<(<(<{}))><([]([]() - Expected >, but found ) instead.\n",
    "<{([([[(<>()){}]>(<<{{ - Expected ], but found > instead.\n",
    "```\n",
    "Stop at the first incorrect closing character on each corrupted line.\n",
    "\n",
    "Did you know that syntax checkers actually have contests to see who can get the high score for syntax errors in a file? It's true! To calculate the syntax error score for a line, take the first illegal character on the line and look it up in the following table:\n",
    "```\n",
    "): 3 points.\n",
    "]: 57 points.\n",
    "}: 1197 points.\n",
    ">: 25137 points.\n",
    "```\n",
    "In the above example, an illegal ) was found twice (2*3 = 6 points), an illegal ] was found once (57 points), an illegal } was found once (1197 points), and an illegal > was found once (25137 points). So, the total syntax error score for this file is 6+57+1197+25137 = 26397 points!\n",
    "\n",
    "Find the first illegal character in each corrupted line of the navigation subsystem. What is the total syntax error score for those errors?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution for part 1 is combined with part 2 (see after part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two\n",
    "Now, discard the corrupted lines. The remaining lines are incomplete.\n",
    "\n",
    "Incomplete lines don't have any incorrect characters - instead, they're missing some closing characters at the end of the line. To repair the navigation subsystem, you just need to figure out the sequence of closing characters that complete all open chunks in the line.\n",
    "\n",
    "You can only use closing characters (), ], }, or >), and you must add them in the correct order so that only legal pairs are formed and all chunks end up closed.\n",
    "\n",
    "In the example above, there are five incomplete lines:\n",
    "```\n",
    "[({(<(())[]>[[{[]{<()<>> - Complete by adding }}]])})].\n",
    "[(()[<>])]({[<{<<[]>>( - Complete by adding )}>]}).\n",
    "(((({<>}<{<{<>}{[]{[]{} - Complete by adding }}>}>)))).\n",
    "{<[[]]>}<{[{[{[]{()[[[] - Complete by adding ]]}}]}]}>.\n",
    "<{([{{}}[<[[[<>{}]]]>[]] - Complete by adding ])}>.\n",
    "```\n",
    "Did you know that autocomplete tools also have contests? It's true! The score is determined by considering the completion string character-by-character. Start with a total score of 0. Then, for each character, multiply the total score by 5 and then increase the total score by the point value given for the character in the following table:\n",
    "```\n",
    "): 1 point.\n",
    "]: 2 points.\n",
    "}: 3 points.\n",
    ">: 4 points.\n",
    "```\n",
    "So, the last completion string above - ])}> - would be scored as follows:\n",
    "```\n",
    "Start with a total score of 0.\n",
    "Multiply the total score by 5 to get 0, then add the value of ] (2) to get a new total score of 2.\n",
    "Multiply the total score by 5 to get 10, then add the value of ) (1) to get a new total score of 11.\n",
    "Multiply the total score by 5 to get 55, then add the value of } (3) to get a new total score of 58.\n",
    "Multiply the total score by 5 to get 290, then add the value of > (4) to get a new total score of 294.\n",
    "The five lines' completion strings have total scores as follows:\n",
    "```\n",
    "```\n",
    "}}]])})] - 288957 total points.\n",
    ")}>]}) - 5566 total points.\n",
    "}}>}>)))) - 1480781 total points.\n",
    "]]}}]}]}> - 995444 total points.\n",
    "])}> - 294 total points.\n",
    "```\n",
    "Autocomplete tools are an odd bunch: the winner is found by sorting all of the scores and then taking the middle score. (There will always be an odd number of scores to consider.) In this example, the middle score is 288957 because there are the same number of scores smaller and larger than it.\n",
    "\n",
    "Find the completion string for each incomplete line, score the completion strings, and sort the scores. What is the middle score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Getting some variables set up for finding the totol error score of the lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = {'(': ')', '[': ']', '{': '}', '<': '>'}\n",
    "errorScoreConversions = {')': 3, ']': 57, '}': 1197, '>': 25137}\n",
    "errorScore = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Getting some variables set up for finding the autocomplete scores for each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoCompleteScoreConversions = {')': 1, ']': 2, '}': 3, '>': 4}\n",
    "autoCompleteScores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Looping through the lines to see if they're corrupted or incomplete and finding the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "    lineIsCorrupted = False\n",
    "    expectedClosings = []\n",
    "\n",
    "    for char in line:\n",
    "        if char in pairs: expectedClosings.insert(0, pairs.get(char))\n",
    "        elif char == expectedClosings[0]: expectedClosings.remove(char)\n",
    "        else: #if expected closings don't match up then the chunk is invalid and the line if corrupted\n",
    "            errorScore += errorScoreConversions.get(char)\n",
    "            lineIsCorrupted = True\n",
    "            break\n",
    "\n",
    "    if not lineIsCorrupted: #if our line is not corrupted, then it must be incomplete\n",
    "        autoCompleteScore = 0\n",
    "        for char in expectedClosings: #we can complete the line by adding on our expected closings\n",
    "            autoCompleteScore *= 5\n",
    "            autoCompleteScore += autoCompleteScoreConversions.get(char)\n",
    "        autoCompleteScores.append(autoCompleteScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the median of the autocomplete scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoCompleteScores.sort()\n",
    "middleScore = autoCompleteScores[int(len(autoCompleteScores)/2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part One Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total error score for these lines is 343863\n"
     ]
    }
   ],
   "source": [
    "print(f'The total error score for these lines is {errorScore}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part Two Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The middle autocomplete score for these lines is 2924734236\n"
     ]
    }
   ],
   "source": [
    "print(f'The middle autocomplete score for these lines is {middleScore}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
